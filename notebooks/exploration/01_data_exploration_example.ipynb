{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Example\n",
    "\n",
    "This notebook demonstrates how to use the data pipeline for exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "import pandas as pd\n",
    "from data_pipeline.core.config import Config\n",
    "from data_pipeline.sources import CSVSource, JSONSource, S3Source\n",
    "from data_pipeline.utils import DataProfiler, SchemaInferrer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config.from_yaml('../../config/environments', 'development')\n",
    "print(f\"Loaded configuration: {config.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CSV source\n",
    "csv_source = CSVSource({\n",
    "    'base_path': '../../data/raw',\n",
    "    'encoding': 'utf-8'\n",
    "})\n",
    "\n",
    "# List available CSV files\n",
    "available_files = csv_source.list_sources()\n",
    "print(\"Available CSV files:\")\n",
    "for file in available_files[:10]:  # Show first 10\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample CSV file (replace with your actual file)\n",
    "try:\n",
    "    # Example: load the first CSV file if available\n",
    "    if available_files:\n",
    "        sample_file = available_files[0]\n",
    "        df = csv_source.read(sample_file)\n",
    "        print(f\"Loaded {len(df)} rows from {sample_file}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        display(df.head())\n",
    "    else:\n",
    "        # Create sample data for demonstration\n",
    "        df = pd.DataFrame({\n",
    "            'id': range(1, 1001),\n",
    "            'name': [f'Customer_{i}' for i in range(1, 1001)],\n",
    "            'age': pd.np.random.randint(18, 80, 1000),\n",
    "            'salary': pd.np.random.randint(30000, 150000, 1000),\n",
    "            'department': pd.np.random.choice(['Sales', 'Marketing', 'Engineering', 'HR'], 1000),\n",
    "            'join_date': pd.date_range('2020-01-01', periods=1000, freq='D'),\n",
    "            'is_active': pd.np.random.choice([True, False], 1000, p=[0.8, 0.2])\n",
    "        })\n",
    "        print(\"Created sample dataset for demonstration\")\n",
    "        display(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Create sample data as fallback\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, 101),\n",
    "        'value': pd.np.random.randn(100),\n",
    "        'category': pd.np.random.choice(['A', 'B', 'C'], 100)\n",
    "    })\n",
    "    print(\"Using fallback sample data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data profiler\n",
    "profiler = DataProfiler({\n",
    "    'sample_size': 10000,\n",
    "    'correlation_threshold': 0.7\n",
    "})\n",
    "\n",
    "# Generate comprehensive profile\n",
    "profile = profiler.profile_dataset(df, \"Sample Dataset\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset Overview:\")\n",
    "info = profile['dataset_info']\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data quality assessment\n",
    "print(\"\\nData Quality Summary:\")\n",
    "quality = profile['data_quality']\n",
    "print(f\"  Overall Score: {quality['overall_score']:.1f}%\")\n",
    "print(f\"  Completeness: {quality['completeness_score']:.1f}%\")\n",
    "print(f\"  Duplicate Rows: {quality['duplicate_rows']} ({quality['duplicate_percentage']:.1f}%)\")\n",
    "\n",
    "if quality['quality_issues']['high_null_columns']:\n",
    "    print(f\"  High Null Columns: {quality['quality_issues']['high_null_columns']}\")\n",
    "if quality['quality_issues']['potential_id_columns']:\n",
    "    print(f\"  Potential ID Columns: {quality['quality_issues']['potential_id_columns']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "for i, rec in enumerate(profile['recommendations'], 1):\n",
    "    print(f\"  {i}. [{rec['priority'].upper()}] {rec['title']}\")\n",
    "    print(f\"     {rec['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize schema inferrer\n",
    "schema_inferrer = SchemaInferrer({\n",
    "    'sample_size': 5000,\n",
    "    'cardinality_threshold': 50\n",
    "})\n",
    "\n",
    "# Infer schema\n",
    "schema = schema_inferrer.infer_schema(df, \"sample_table\")\n",
    "\n",
    "print(f\"Schema for table: {schema['table_name']}\")\n",
    "print(f\"Total columns: {schema['total_columns']}\")\n",
    "print(f\"Total rows analyzed: {schema['total_rows']}\")\n",
    "print(f\"Sample rows: {schema['sample_rows']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column analysis\n",
    "print(\"\\nColumn Analysis:\")\n",
    "for col_name, col_info in schema['columns'].items():\n",
    "    print(f\"\\n{col_name}:\")\n",
    "    print(f\"  Type: {col_info['pandas_dtype']} -> {col_info['inferred_sql_type']}\")\n",
    "    print(f\"  Nullable: {col_info['nullable']} ({col_info['null_percentage']:.1f}% nulls)\")\n",
    "    print(f\"  Unique values: {col_info['unique_count']} ({col_info['cardinality']} cardinality)\")\n",
    "    print(f\"  Quality score: {col_info['data_quality_score']:.1f}%\")\n",
    "    \n",
    "    if col_info['sample_values']:\n",
    "        print(f\"  Sample values: {col_info['sample_values'][:5]}\")\n",
    "    \n",
    "    if col_info['patterns']:\n",
    "        print(f\"  Detected patterns: {col_info['patterns']}\")\n",
    "        \n",
    "    if col_info['anomalies']:\n",
    "        print(f\"  Anomalies: {col_info['anomalies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recommendations from schema inference\n",
    "print(\"\\nDatabase Recommendations:\")\n",
    "\n",
    "if schema['primary_key_candidates']:\n",
    "    print(f\"Primary Key Candidates: {schema['primary_key_candidates']}\")\n",
    "\n",
    "if schema['foreign_key_candidates']:\n",
    "    print(\"Foreign Key Candidates:\")\n",
    "    for fk in schema['foreign_key_candidates']:\n",
    "        print(f\"  - {fk['column']} -> {fk['referenced_table']} (confidence: {fk['confidence']})\")\n",
    "\n",
    "if schema['constraints']['not_null']:\n",
    "    print(f\"NOT NULL constraints recommended: {schema['constraints']['not_null']}\")\n",
    "\n",
    "if schema['indexes_recommended']:\n",
    "    print(\"\\nIndex Recommendations:\")\n",
    "    for idx in schema['indexes_recommended']:\n",
    "        print(f\"  - {idx['type']} index on {idx['columns']} (reason: {idx['reason']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate HTML report\n",
    "report_path = profiler.generate_html_report(profile, \"../../reports/sample_data_profile.html\")\n",
    "print(f\"HTML report generated: {report_path}\")\n",
    "print(\"Open this file in a web browser to view the detailed report.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Data Cleaning**: Based on the profiling results, clean the data using the processors\n",
    "2. **Schema Creation**: Use the inferred schema to create database tables\n",
    "3. **Data Validation**: Set up validation rules using Great Expectations\n",
    "4. **Pipeline Automation**: Create YAML configurations for automated processing\n",
    "5. **Monitoring**: Set up monitoring and alerting for data quality issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}